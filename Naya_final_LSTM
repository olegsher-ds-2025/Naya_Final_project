{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":22073,"sourceType":"datasetVersion","datasetId":16752}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import butter, lfilter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"ticks\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader,  Dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.utils.class_weight import compute_class_weight\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get data from csv and create df","metadata":{}},{"cell_type":"code","source":"# change these following three lines only\nsubject_data_file = 'data_subjects_info.csv'\ndata_dir = '/kaggle/input/motionsense-dataset/A_DeviceMotion_data'\n\nos.chdir(data_dir)\nos.chdir(os.pardir)\n\ndef get_all_dataset_paths(input_dir) -> []:\n    input_files = []\n    for dirs, subdirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith('.csv'):\n                input_files.append(os.path.join(dirs, file))\n    return input_files\n\ndef load_whole_dataframe_from_paths(paths, meta) -> pd.DataFrame:\n    \n    df = pd.DataFrame()\n\n    for p in paths:\n        c_dir, c_file = p.split('/')[-2], p.split('/')[-1]\n        c_cat, c_ses = c_dir.split('_')[-2], c_dir.split('_')[-1]\n        c_sub = c_file.split('_')[-1].split('.')[-2]\n        \n        tdf = pd.read_csv(p, encoding = \"utf-8\")\n        tdf = tdf.assign(person_id = int(c_sub))\n        tdf = tdf.assign(trial_id = int(c_ses))\n        tdf = tdf.assign(act = str(c_cat))\n        tdf = tdf.assign(age = int(meta.age[int(c_sub) - 1]))\n        tdf = tdf.assign(gender = int(meta.gender[int(c_sub) - 1]))\n        tdf = tdf.assign(height = int(meta.height[int(c_sub) - 1]))\n        tdf = tdf.assign(weight = int(meta.weight[int(c_sub) - 1]))\n\n        df = pd.concat([df, tdf])\n    \n    df.reset_index(drop=True, inplace=True)\n    return df\n\nsubject_data_frame = pd.DataFrame(pd.read_csv(subject_data_file, encoding = \"utf-8\"))\nall_dataset_paths = get_all_dataset_paths(data_dir)\ndata_frame = load_whole_dataframe_from_paths(all_dataset_paths, subject_data_frame)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer activity to numeric","metadata":{}},{"cell_type":"code","source":"lEncoder = LabelEncoder()\nlabels = lEncoder.fit(data_frame.act)\ndata_frame['activity'] = lEncoder.transform(data_frame.act)\n# data_frame.drop('act', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_frame[['act', 'activity']].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_frame.drop('act', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = ['attitude.roll',\n    'attitude.pitch',\n    'attitude.yaw',\n    'gravity.x',\n    'gravity.y',\n    'gravity.z',\n    'rotationRate.x',\n    'rotationRate.y',\n    'rotationRate.z',\n    'userAcceleration.x',\n    'userAcceleration.y',\n    'userAcceleration.z'\n           ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target = ['activity'] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data understanding","metadata":{}},{"cell_type":"code","source":"data_frame.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Number of Samples per Person","metadata":{}},{"cell_type":"code","source":"person_counts = data_frame['person_id'].value_counts().sort_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=person_counts.index, y=person_counts.values, palette=\"viridis\")\nplt.title(\"Number of Samples per Person\")\nplt.xlabel(\"Person\")\nplt.ylabel(\"Sample Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Samples per Trial for Each Person","metadata":{}},{"cell_type":"code","source":"grouped = data_frame.groupby(['person_id', 'trial_id'])['activity'].count().reset_index(name='count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.barplot(data=grouped, x='person_id', y='count', hue='trial_id', palette='tab20')\nplt.title(\"Samples per Trial for Each Person\")\nplt.xlabel(\"Person\")\nplt.ylabel(\"Sample Count per Trial\")\nplt.legend(title='Trial', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Heatmap of Sample Count per Person-Trial","metadata":{}},{"cell_type":"code","source":"pivot_df = grouped.pivot(index='person_id', columns='trial_id', values='count')\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\nplt.title(\"Heatmap of Sample Count per Person-Trial\")\nplt.xlabel(\"Trial\")\nplt.ylabel(\"Person\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Number of Samples per Activity","metadata":{}},{"cell_type":"markdown","source":"wlk  -> 5 <br />\nsit  -> 2 <br />\nstd  -> 3 <br />\nups  -> 4 <br />\njog  -> 1 <br />\ndws  -> 0 <br />","metadata":{}},{"cell_type":"code","source":"activity_counts = data_frame['activity'].value_counts().sort_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=activity_counts.index, y=activity_counts.values, palette=\"viridis\")\nplt.title(\"Number of Samples per Activity\")\nplt.xlabel(\"Activity\")\nplt.ylabel(\"Sample Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped = data_frame.groupby(['person_id', 'activity'])['activity'].count().reset_index(name='count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pivot_df = grouped.pivot(index='person_id', columns='activity', values='count')\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\nplt.title(\"Heatmap of Sample Count per Person-Activity\")\nplt.xlabel(\"Activity\")\nplt.ylabel(\"Person\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grouping activity\\person per trial","metadata":{}},{"cell_type":"code","source":"grouped = data_frame.groupby(['activity', 'person_id'])['trial_id'].count().reset_index(name='count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Samples per Activity for Each Person","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.barplot(data=grouped, x='activity', y='count', hue='person_id', palette='tab20')\nplt.title(\"Samples per Activity for Each Person\")\nplt.xlabel(\"Activity\")\nplt.ylabel(\"Sample Count per Activity\")\nplt.legend(title='Trial', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subject_data_frame.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import seaborn.objects as so\n# (\n#     so.Plot(data_frame, x=\"person_id\", y=\"weight\", color=\"age\")\n#     .add(so.Dot(pointsize=10), so.Agg())\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Heatmap of Sensors - activity","metadata":{}},{"cell_type":"code","source":"# grouped = data_frame.sample(10000).groupby(['activity', 'attitude.roll'])['attitude.roll'].count().reset_index(name='count')\n# pivot_df = grouped.pivot(index='activity', columns='attitude.roll', values='count')\n# plt.figure(figsize=(10, 6))\n# sns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\n# plt.title(\"Heatmap of Sample Count per attitude.roll-Activity\")\n# plt.ylabel(\"Activity\")\n# plt.xlabel(\"attitude.roll\")\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grouped","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pivot_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grouped = data_frame.sample(10000).groupby(['activity', 'gravity.x'])['gravity.x'].count().reset_index(name='count')\n# pivot_df = grouped.pivot(index='activity', columns='gravity.x', values='count')\n# plt.figure(figsize=(10, 6))\n# sns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\n# plt.title(\"Heatmap of Sample Count per gravity.x\")\n# plt.ylabel(\"Activity\")\n# plt.xlabel(\"gravity.x\")\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grouped = data_frame.sample(10000).groupby(['activity', 'gravity.y'])['gravity.y'].count().reset_index(name='count')\n# pivot_df = grouped.pivot(index='activity', columns='gravity.y', values='count')\n# plt.figure(figsize=(10, 6))\n# sns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\n# plt.title(\"Heatmap of Sample Count per gravity.y\")\n# plt.ylabel(\"Activity\")\n# plt.xlabel(\"gravity.y\")\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grouped = data_frame.sample(10000).groupby(['activity', 'gravity.z'])['gravity.z'].count().reset_index(name='count')\n# pivot_df = grouped.pivot(index='activity', columns='gravity.z', values='count')\n# plt.figure(figsize=(10, 6))\n# sns.heatmap(pivot_df, annot=True, fmt='g', cmap='YlGnBu')\n# plt.title(\"Heatmap of Sample Count per gravity.z\")\n# plt.xlabel(\"gravity.z\")\n# plt.ylabel(\"activity\")\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data optimization","metadata":{}},{"cell_type":"markdown","source":"## Aggregate features per trial","metadata":{}},{"cell_type":"code","source":"# Aggregate features (mean, std, etc.) per person/trial\nagg_df = data_frame.groupby(['person_id', 'trial_id'])[features].agg(['mean', 'std', 'min', 'max']).reset_index()\n\n# Flatten multi-index columns\nagg_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_df.columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agg_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PSA","metadata":{}},{"cell_type":"code","source":"# X = agg_df.drop(columns=['person_id_', 'trial_id_'])\n\n# pca = PCA(n_components=2)\n# X_pca = pca.fit_transform(X)\n\n# # Add back labels\n# agg_df['pca1'] = X_pca[:, 0]\n# agg_df['pca2'] = X_pca[:, 1]\n\n# plt.figure(figsize=(10, 6))\n# sns.scatterplot(data=agg_df, x='pca1', y='pca2', hue='person_id_', palette='tab10', s=60)\n# plt.title(\"PCA Projection of Trial Features\")\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## t-SNE","metadata":{}},{"cell_type":"code","source":"\n# tsne = TSNE(n_components=2, perplexity=20, random_state=42)\n# X_tsne = tsne.fit_transform(X)\n\n# agg_df['tsne1'] = X_tsne[:, 0]\n# agg_df['tsne2'] = X_tsne[:, 1]\n\n# plt.figure(figsize=(10, 6))\n# sns.scatterplot(data=agg_df, x='tsne1', y='tsne2', hue='person_id_', palette='tab10', s=60)\n# plt.title(\"t-SNE of Trial Features\")\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Signaling","metadata":{}},{"cell_type":"code","source":"def load_signal(signal_column, person, trial, activity):\n    return data_frame[(data_frame['person_id'] == person) & \n                      (data_frame['trial_id'] == trial) & \n                      (data_frame['activity'] == activity)][signal_column].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Preprocess Signal\ndef preprocess_signal(signal):    \n    # Subtract the mean to remove the DC component\n    signal = signal - np.mean(signal)\n    \n    # Normalize the signal between -1 and 1\n    signal_min, signal_max = np.min(signal), np.max(signal)\n    return  2 * (signal - signal_min) / (signal_max - signal_min) - 1\n    # return normalized_signal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_filter(signal, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order)\n    filtered_signal = lfilter(b, a, signal)\n    return filtered_signal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_signal(signal, fs):\n    n = len(signal)\n    f = np.fft.rfftfreq(n, 1/fs)\n    fft_magnitude = np.abs(np.fft.rfft(signal))\n    return f, fft_magnitude","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def butter_lowpass(cutoff, fs, order=5):\n    nyquist = 0.5 * fs\n    normal_cutoff = cutoff / nyquist\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Visualize and Save Results\ndef visualize_signal(original_signal, processed_signal, fft_freq, fft_magnitude, data_title):\n    plt.figure(figsize=(12, 8))\n    \n    # Set the overall title for the entire figure\n    plt.suptitle(data_title, fontsize=16)\n    \n    # Original Signal\n    plt.subplot(3, 1, 1)\n    plt.plot(original_signal)\n    plt.title(\"Original Signal\")\n    plt.xlabel(\"Time (samples)\")\n    plt.ylabel(\"Amplitude\")\n\n    # Filtered Signal\n    plt.subplot(3, 1, 2)\n    plt.plot(processed_signal, color='orange')\n    plt.title(\"Filtered Signal\")\n    plt.xlabel(\"Time (samples)\")\n    plt.ylabel(\"Amplitude\")\n\n# FFT Magnitude\n    plt.subplot(3, 1, 3)\n    plt.plot(fft_freq, fft_magnitude, color='green')\n    plt.title(\"Frequency Spectrum\")\n    plt.xlabel(\"Frequency (Hz)\")\n    plt.ylabel(\"Magnitude\")\n\n    # Add a vertical line at the cutoff frequency indicating the point (5oHz) where the filter started reducing the other frequencies above it\n    plt.axvline(x=cutoff_freq, color='red', linestyle='--', label=f'Cutoff: {cutoff_freq} Hz')\n\n\n    # Apply logarithmic scale if needed to y-axis for better visualization\n    plt.yscale('log')\n\n    # Set frequency limit if necessary (example: show only up to 80 Hz)\n    plt.xlim(0, 80)  # Adjust the limit based on your data\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"wlk  -> 5 trials: 7,8,15<br />\nsit  -> 2 trials: 5,13<br />\nstd  -> 3 trials: 6,14<br />\nups  -> 4 trials: 3,4,12<br />\njog  -> 1 trials: 9,16<br />\ndws  -> 0 trials: 1,2,11<br />","metadata":{}},{"cell_type":"code","source":"cutoff_freq = 50  # Hz\nsampling_rate = 1000  # Hz\ncolumn_to_process = 'attitude.pitch'\ndataset_type = 'Sensor Data'\nperson = 1\n# trial = 1\ndws = {0: [1,2,11]}\n\n# for k,v in dws.items():\n#     for _ in v:\n#         signal = load_signal(column_to_process, person, _, k)\n#         normalized_signal = preprocess_signal(signal)\n#         filtered_signal = apply_filter(normalized_signal, cutoff_freq, sampling_rate)\n#         fft_freq, fft_magnitude = analyze_signal(filtered_signal, sampling_rate)\n\n#         data_title = f\"{dataset_type}: {column_to_process.upper()} Person: {person} Activity: DWS {0} Trial: {_}\"\n#         visualize_signal(signal, filtered_signal, fft_freq, fft_magnitude, data_title)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wlk = {5 : [7,8,15]}\n\n# for k,v in wlk.items():\n#     for _ in v:\n#         signal = load_signal(column_to_process, person, _, k)\n#         normalized_signal = preprocess_signal(signal)\n#         filtered_signal = apply_filter(normalized_signal, cutoff_freq, sampling_rate)\n#         fft_freq, fft_magnitude = analyze_signal(filtered_signal, sampling_rate)\n\n#         data_title = f\"{dataset_type}: {column_to_process.upper()} Person: {person} Activity: WLK {k} Trial: {_}\"\n#         visualize_signal(signal, filtered_signal, fft_freq, fft_magnitude, data_title)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ups = {4 : [3,4,12]}\n\n# for k,v in ups.items():\n#     for _ in v:\n#         signal = load_signal(column_to_process, person, _, k)\n#         normalized_signal = preprocess_signal(signal)\n#         filtered_signal = apply_filter(normalized_signal, cutoff_freq, sampling_rate)\n#         fft_freq, fft_magnitude = analyze_signal(filtered_signal, sampling_rate)\n\n#         data_title = f\"{dataset_type}: {column_to_process.upper()} Person: {person} Activity: UPS {k} Trial: {_}\"\n#         visualize_signal(signal, filtered_signal, fft_freq, fft_magnitude, data_title)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Debugging = set dataset to small 10000 sample data for debug(speed)","metadata":{}},{"cell_type":"code","source":"# 1412865 max in dataset\n# 1412800 - max what work without error in model\n# df = data_frame.head(1412864).copy()\ndf = data_frame.sample(1000).copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"seq_len = 64 # use 28.5G memory - max available !!!\nepoch_number = 5 # moved from 5 to 2 becouse overfitting \nbatch_size = 64 # 32\nnum_layers = 4 # 1\ntest_size = 0.2\nhidden_size = 64 \noutput_size = 6 \n# 6 - wlk -> 5 trials: 7,8,15 sit -> 2 trials: 5,13  std -> 3 trials: 6,14  ups -> 4 trials: 3,4,12 jog -> 1 trials: 9,16  dws -> 0 trials: 1,2,11\ninput_size = len(features)\n# all 12 sensors data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[features])\ny = df[target].values\n\n# Build sequences BEFORE splitting\nX_seq = []\ny_seq = []\n\nfor i in range(len(df) - seq_len):\n    X_seq.append(X_scaled[i:i+seq_len])\n    y_seq.append(y[i+seq_len])\n\nX_seq = np.array(X_seq)   # shape: (samples, seq_len, features)\ny_seq = np.array(y_seq)   # shape: (samples,)\n\n# Train / Test Split correctly on sequences\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=test_size, shuffle=False)\n\n# Convert to torch tensors\ntrain_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\ntest_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------\n# LSTM Model\n# ------------------------------\nclass LSTMModel(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, num_layers=num_layers):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        _, (hn, _) = self.lstm(x)       # hn: [num_layers, batch, hidden_size]\n        out = self.fc(hn[-1])           # out: [batch, output_size]\n        return out\n        \nmodel = LSTMModel(input_size=input_size, hidden_size=hidden_size).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CrossEntropyLoss Multi-Class Classification\t[batch, classes]\t[batch]\tnn.CrossEntropyLoss()","metadata":{}},{"cell_type":"code","source":"y_train = np.array(y_train).flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, device=device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for xb, yb in dataloader:\n            xb = xb.to(device)\n            yb = yb.squeeze().long().to(device)  \n            outputs = model(xb)                  \n            loss = criterion(outputs, yb)\n            total_loss += loss.item() * xb.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == yb).sum().item()\n            total += yb.size(0)\n\n            all_preds.append(preds.cpu())\n            all_labels.append(yb.cpu())\n\n\n    avg_loss = total_loss / total if total > 0 else 0.0\n    accuracy = correct / total if total > 0 else 0.0\n\n    return avg_loss, accuracy, torch.cat(all_preds), torch.cat(all_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test = y_test.reshape(-1)\ny_train = y_train.reshape(-1)\nfor epoch in range(epoch_number):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs.to(device))\n        labels = labels.squeeze().long().to(device)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(\"Output shape:\", outputs.shape)       # should be [batch_size, 6]\n    print(\"Target shape:\", labels.shape)        # should be [batch_size]\n    print(\"Unique labels:\", labels.unique())    # should be 0 to 5 only\n    print(\"First output logits:\", outputs[0])   # should look like raw scores\n    avg_train_loss = running_loss / len(train_loader)\n    val_loss, val_acc, val_preds, val_labels = evaluate(model, test_loader, criterion, device)\n    print(f\"-> Epoch {epoch+1}/{epoch_number} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.6f} | Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loss:\", loss.item())\nprint(\"Running Loss:\", running_loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if isinstance(val_labels, torch.Tensor):\n    val_labels = val_labels.cpu().numpy()\n\nif isinstance(val_preds, torch.Tensor):\n    val_preds = val_preds.cpu().numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(val_labels, val_preds)\nConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}